########################################################################################
###  INPUT LAYER
########################################################################################
# Input for Training

layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param { 
	shape: { 
		dim: 1 
		dim: 3 
		dim: 32 
		dim: 32
	}
  }
}

########################################################################################
###  CONVOLUTIONAL LAYER 1
########################################################################################
layer {
	name: "conv1"
	type: "Convolution"
	bottom: "data"
	top: "conv1_1"
	param {
		lr_mult : 1 
		##decay_mult: 1
	}
	param {
		lr_mult : 2
		##decay_mult: 0
	}
	convolution_param {
		num_output: 32
		pad: 1
		kernel_size: 3
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}	

# RELU layer for Layer - 1
layer {
	name: "relu1"
	type: "ReLU"
	bottom: "conv1_1"
	top: "conv1_1"
}

# Max pooling layer for Layer -1
layer {
	name: "max1"
	type: "Pooling"
	bottom: "conv1_1"
	top: "pool1"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}

########################################################################################
###  CONVOLUTIONAL LAYER 2
########################################################################################
layer {
	name: "conv2"
	type: "Convolution"
	bottom: "pool1"
	top: "conv2_1"
	param {
		lr_mult : 1 
		#decay_mult: 1
	}
	param {
		lr_mult : 2
		#decay_mult: 0
	}
	convolution_param {
		num_output: 64
		pad: 1
		kernel_size: 3
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}	

# RELU layer for Layer - 2
layer {
	name: "relu2"
	type: "ReLU"
	bottom: "conv2_1"
	top: "conv2_1"
}

# Max pooling layer for Layer -2
layer {
	name: "max2"
	type: "Pooling"
	bottom: "conv2_1"
	top: "pool2"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}
########################################################################################
###  CONVOLUTIONAL LAYER 3
########################################################################################
layer {
	name: "conv3"
	type: "Convolution"
	bottom: "pool2"
	top: "conv3_1"
	param {
		lr_mult : 1 
		#decay_mult: 1
	}
	param {
		lr_mult : 2
		#decay_mult: 0
	}
	convolution_param {
		num_output: 128
		pad: 1
		kernel_size: 3
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}	

# RELU layer for Layer - 3
layer {
	name: "relu3"
	type: "ReLU"
	bottom: "conv3_1"
	top: "conv3_1"
}

# Max pooling layer for Layer -3
layer {
	name: "max3"
	type: "Pooling"
	bottom: "conv3_1"
	top: "pool3"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}
#layer{
#	name: "flatdata"
#	type: "Flatten"
#	bottom: "pool3"
#	top: "flatdata"
#}
########################################################################################
###  FULLY-CONNECTED LAYER -1
########################################################################################
layer{
	name: "fc1"
	type: "InnerProduct"
	bottom: "pool3"
	top: "fc1"
	param {
		lr_mult : 1 
		#decay_mult: 1
	}
	param {
		lr_mult : 2
		#decay_mult: 0
	}
	inner_product_param {
		num_output: 1000
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}
# RELU layer for Layer - 4
layer {
	name: "relu4"
	type: "ReLU"
	bottom: "fc1"
	top: "fc1"
}
# Dropout layer
layer{
	name: "drop1"
	type: "Dropout"
	bottom: "fc1"
	top: "fc1"
	dropout_param {
		dropout_ratio: 0.5
	}
}

########################################################################################
###  FULLY-CONNECTED LAYER -2
########################################################################################
layer{
	name: "fc2"
	type: "InnerProduct"
	bottom: "fc1"
	top: "fc2"
	param {
		lr_mult : 1 
		#decay_mult: 1
	}
	param {
		lr_mult : 2
		#decay_mult: 0
	}
	inner_product_param {
		num_output: 1000
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}
# RELU layer for Layer - 5
layer {
	name: "relu5"
	type: "ReLU"
	bottom: "fc2"
	top: "fc2"
}
# Dropout layer
layer{
	name: "drop2"
	type: "Dropout"
	bottom: "fc2"
	top: "fc2"
	dropout_param {
		dropout_ratio: 0.5
	}
}
########################################################################################
###  FULLY-CONNECTED LAYER -3
########################################################################################
layer{
	name: "fc3"
	type: "InnerProduct"
	bottom: "fc2"
	top: "output"
	param {
		lr_mult : 1 
		#decay_mult: 1
	}
	param {
		lr_mult : 2
		#decay_mult: 0
	}
	inner_product_param {
		num_output: 5
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}


########################################################################################
###  SOFTMAX LAYERS
########################################################################################
layer {

	bottom: "output"
	top: "prob"
	name: "prob"	
	type: "Softmax"  			
}
