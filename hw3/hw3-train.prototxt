########################################################################################
###  INPUT LAYER
########################################################################################
# Input for Training

layer {
	name: "data"
	type: "ImageData"
	top: "data"
	top: "label"
	image_data_param {	
		source: "/home/min/a/das35/ee570/hw3/hw3-train-split.txt"
		new_height: 32
		new_width: 32
		batch_size: 30
		shuffle: true
	}
	#transform_param {
	#    scale: 0.00369
	#}
	include {
		phase: TRAIN	
	}
}

# Input for Testing
layer {
	name: "data"
	type: "ImageData"
	top: "data"
	top: "label"
	image_data_param {
		source: "/home/min/a/das35/ee570/hw3/hw3-test-split.txt"
		new_height: 32
		new_width: 32
		batch_size: 32
		shuffle: true
	}
	#transform_param {
	#    scale: 0.00369
	#}
	include {
		phase: TEST	
	}
}

########################################################################################
###  CONVOLUTIONAL LAYER 1
########################################################################################
layer {
	name: "conv1"
	type: "Convolution"
	bottom: "data"
	top: "conv1_1"
	param {
		lr_mult : 1 
		##decay_mult: 1
	}
	param {
		lr_mult : 2
		##decay_mult: 0
	}
	convolution_param {
		num_output: 32
		pad: 1
		kernel_size: 3
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}	

# RELU layer for Layer - 1
layer {
	name: "relu1"
	type: "ReLU"
	bottom: "conv1_1"
	top: "conv1_1"
}

# Max pooling layer for Layer -1
layer {
	name: "max1"
	type: "Pooling"
	bottom: "conv1_1"
	top: "pool1"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}

########################################################################################
###  CONVOLUTIONAL LAYER 2
########################################################################################
layer {
	name: "conv2"
	type: "Convolution"
	bottom: "pool1"
	top: "conv2_1"
	param {
		lr_mult : 1 
		#decay_mult: 1
	}
	param {
		lr_mult : 2
		#decay_mult: 0
	}
	convolution_param {
		num_output: 64
		pad: 1
		kernel_size: 3
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}	

# RELU layer for Layer - 2
layer {
	name: "relu2"
	type: "ReLU"
	bottom: "conv2_1"
	top: "conv2_1"
}

# Max pooling layer for Layer -2
layer {
	name: "max2"
	type: "Pooling"
	bottom: "conv2_1"
	top: "pool2"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}
########################################################################################
###  CONVOLUTIONAL LAYER 3
########################################################################################
layer {
	name: "conv3"
	type: "Convolution"
	bottom: "pool2"
	top: "conv3_1"
	param {
		lr_mult : 1 
		#decay_mult: 1
	}
	param {
		lr_mult : 2
		#decay_mult: 0
	}
	convolution_param {
		num_output: 128
		pad: 1
		kernel_size: 3
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}	

# RELU layer for Layer - 3
layer {
	name: "relu3"
	type: "ReLU"
	bottom: "conv3_1"
	top: "conv3_1"
}

# Max pooling layer for Layer -3
layer {
	name: "max3"
	type: "Pooling"
	bottom: "conv3_1"
	top: "pool3"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}
#layer{
#	name: "flatdata"
#	type: "Flatten"
#	bottom: "pool3"
#	top: "flatdata"
#}
########################################################################################
###  FULLY-CONNECTED LAYER -1
########################################################################################
layer{
	name: "fc1"
	type: "InnerProduct"
	bottom: "pool3"
	top: "fc1"
	param {
		lr_mult : 1 
		#decay_mult: 1
	}
	param {
		lr_mult : 2
		#decay_mult: 0
	}
	inner_product_param {
		num_output: 1000
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}
# RELU layer for Layer - 4
layer {
	name: "relu4"
	type: "ReLU"
	bottom: "fc1"
	top: "fc1"
}
# Dropout layer
layer{
	name: "drop1"
	type: "Dropout"
	bottom: "fc1"
	top: "fc1"
	dropout_param {
		dropout_ratio: 0.5
	}
}

########################################################################################
###  FULLY-CONNECTED LAYER -2
########################################################################################
layer{
	name: "fc2"
	type: "InnerProduct"
	bottom: "fc1"
	top: "fc2"
	param {
		lr_mult : 1 
		#decay_mult: 1
	}
	param {
		lr_mult : 2
		#decay_mult: 0
	}
	inner_product_param {
		num_output: 1000
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}
# RELU layer for Layer - 5
layer {
	name: "relu5"
	type: "ReLU"
	bottom: "fc2"
	top: "fc2"
}
# Dropout layer
layer{
	name: "drop2"
	type: "Dropout"
	bottom: "fc2"
	top: "fc2"
	dropout_param {
		dropout_ratio: 0.5
	}
}
########################################################################################
###  FULLY-CONNECTED LAYER -3
########################################################################################
layer{
	name: "fc3"
	type: "InnerProduct"
	bottom: "fc2"
	top: "output"
	param {
		lr_mult : 1 
		#decay_mult: 1
	}
	param {
		lr_mult : 2
		#decay_mult: 0
	}
	inner_product_param {
		num_output: 5
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
	}
}


########################################################################################
###  SOFTMAX LAYERS
########################################################################################
### Softmax with loss (for training)
layer {
	name: "loss"				
	type: "SoftmaxWithLoss"		
	bottom: "output"
	bottom: "label"				
	top: "prob"				
	include {
		phase: TRAIN
	}
}
### Softmax without loss (for testing)
layer {

	bottom: "output"
	top: "prob"
	name: "prob"	
	type: "Softmax"  			
	include {
		phase: TEST
	}
}
########################################################################################
###  ACCURACY LAYER
########################################################################################
layer {
	name: "accuracy"
	type: "Accuracy"			
	bottom: "prob"
	bottom: "label"
	top: "accuracy/top-1"
	include {
		phase: TEST
	}
}



